# Explanatory Interactive Learning for Bias Mitigation in Gender Classification

This repository contains the complete implementation of the XIL (Explanatory Interactive Learning) framework for bias mitigation in visual gender classification, as described in the research paper.

## Overview

We propose a comprehensive framework that combines multiple state-of-the-art techniques for bias-aware machine learning:

### ğŸ”¬ **Core XIL Components**
- **CAIPI (Counterfactual Augmentation)** - Data augmentation with counterfactual transformations
- **BLA (Bounded Logit Attention)** - Self-explaining neural networks with attention mechanisms  
- **RRR (Right-for-Right-Reasons)** - Regularization using explanation-guided training
- **Hybrid Training** - Combined CAIPI + RRR approach for enhanced bias mitigation

### ğŸ“Š **Bias Evaluation Metrics**
- **FFP (Foreground Focus Proportion)** - Measures focus on relevant foreground features
- **BFP (Background Focus Proportion)** - Quantifies attention to spurious background features  
- **BSR (Background Saliency Ratio)** - Ratio of background to foreground saliency
- **DICE score** - Measures the alignment between two masks

## Project Structure

```
explanatory_gender_classification/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ dataset.py          # Gender dataset with mask support
â”‚   â”‚   â””â”€â”€ preprocessing.py    # Data preprocessing utilities
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ architectures.py    # CNN model factory
â”‚   â”‚   â”œâ”€â”€ cnn_models.py      # Model compatibility wrappers
â”‚   â”‚   â”œâ”€â”€ rrr_model.py       # Right-for-Right-Reasons models
â”‚   â”‚   â””â”€â”€ gender_classifier.py # Base gender classification models
â”‚   â”œâ”€â”€ explainability/
â”‚   â”‚   â”œâ”€â”€ gradcam.py         # GradCAM explanations
â”‚   â”‚   â”œâ”€â”€ bla.py             # Bounded Logit Attention
â”‚   â”‚   â””â”€â”€ lime_wrapper.py     # LIME explanations
â”‚   â”œâ”€â”€ augmentation/
â”‚   â”‚   â””â”€â”€ caipi.py           # CAIPI counterfactual augmentation
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ trainer.py         # Base training classes
â”‚   â”‚   â”œâ”€â”€ rrr_trainer.py     # RRR-specific training
â”‚   â”‚   â””â”€â”€ hybrid_trainer.py   # CAIPI + RRR hybrid training
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”œâ”€â”€ bias_metrics.py    # FFP, BFP, BSR, DICE metrics
â”‚   â”‚   â””â”€â”€ evaluator.py       # Model evaluation pipeline
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ helpers.py         # Utility functions
â”‚       â””â”€â”€ settings.py        # Configuration constants
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_baseline.py      # Train baseline CNN models
â”‚   â”œâ”€â”€ train_caipi.py         # Train CAIPI-augmented models
â”‚   â”œâ”€â”€ train_rrr.py           # Train RRR models
â”‚   â”œâ”€â”€ train_hybrid.py        # Train hybrid XIL models
â”‚   â”œâ”€â”€ run_all_experiments.py # Complete experimental suite
â”‚   â”œâ”€â”€ evaluate_models.py     # Model evaluation
â”‚   â””â”€â”€ generate_explanations.py # Generate visual explanations
â”œâ”€â”€ experiments/               # Experimental results and analysis
â”œâ”€â”€ tests/                    # Unit tests
â””â”€â”€ notebooks/               # Jupyter notebooks for analysis
```

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone <repository-url>
cd explanatory_gender_classification

# Install dependencies
pip install -r requirements.txt
```

### Test Installation

```bash
# Test all XIL components
python test_implementation.py
```

Expected output:
```
âœ“ All imports successful!
âœ“ BLA Model: PASSED
âœ“ CAIPI Augmentation: PASSED  
âœ“ Bias Metrics: PASSED
âœ“ Hybrid Training: PASSED
ğŸ‰ All tests passed! The XIL implementation is ready.
```

## Data Setup

This project expects your gender classification dataset to be organized in one of two ways:

### Option 1: Gender Dataset Folder (Recommended)
For the complete structured dataset with pre-splits and masks:
```
gender_dataset/
â”œâ”€â”€ dataset_split/
â”‚   â”œâ”€â”€ train_set.csv
â”‚   â”œâ”€â”€ val_set.csv
â”‚   â””â”€â”€ test_set.csv
â”œâ”€â”€ resized_female_images/
â”‚   â”œâ”€â”€ image1.jpg
â”‚   â”œâ”€â”€ image2.jpg
â”‚   â””â”€â”€ ...
â”œâ”€â”€ resized_male_images/
â”‚   â”œâ”€â”€ image1.jpg
â”‚   â”œâ”€â”€ image2.jpg
â”‚   â””â”€â”€ ...
â”œâ”€â”€ resized_female_masks/
â”‚   â”œâ”€â”€ image1.png
â”‚   â”œâ”€â”€ image2.png
â”‚   â””â”€â”€ ...
â””â”€â”€ resized_male_masks/
    â”œâ”€â”€ image1.png
    â”œâ”€â”€ image2.png
    â””â”€â”€ ...
```

### Option 2: Separate Directories
You can also provide separate paths to female and male image directories for simpler setups.

## ğŸ’» Usage

### 1. Baseline CNN Training
```bash
# Train standard CNN without XIL
python scripts/train_baseline.py --gender_dataset_path ../gender_dataset --model efficientnet_b0 --epochs 20
```

### 2. CAIPI Augmentation Training  
```bash
# Train with counterfactual augmentation
python scripts/train_caipi.py --data_dir ../gender_dataset --k 3 --sampling uncertainty --explainer gradcam
```

### 3. RRR (Right-for-Right-Reasons) Training
```bash
# Train with explanation-guided regularization
python scripts/train_rrr.py --gender_dataset_path ../gender_dataset --model efficientnet_b0 --l2_grads 1000
```

### 4. Hybrid XIL Training (CAIPI + RRR)
```bash
# Train with combined CAIPI and RRR approach  
python scripts/train_hybrid.py --data_dir ../gender_dataset --k 3 --sampling uncertainty --explainer gradcam --rrr_lambda 10.0
```

### 5. Complete Experimental Suite (28 Experiments)
```bash
# Run all experiments from the paper
python scripts/run_all_experiments.py --data_dir ../gender_dataset
```

This runs:
- **6 Baseline experiments** (6 architectures)
- **6 CAIPI experiments** (uncertainty/confident Ã— k={1,3,5})  
- **4 RRR experiments** (uncertainty/confident Ã— GradCAM/BLA)
- **12 Hybrid experiments** (uncertainty/confident Ã— k={1,3,5} Ã— GradCAM/BLA)

### 6. Model Evaluation & Analysis
```bash
# Evaluate trained models with bias metrics
python scripts/evaluate_models.py --model_path experiments/hybrid/best_model.pt

# Generate visual explanations
python scripts/generate_explanations.py --method gradcam --model_path experiments/hybrid/best_model.pt
```

## âš™ï¸ Configuration Options

### Hybrid Training Parameters
```bash
--k                    # CAIPI counterexamples per sample (1, 3, 5)
--sampling            # Strategy: 'uncertainty' or 'high_confidence' 
--explainer           # Method: 'gradcam' or 'bla'
```

## ğŸ”§ Troubleshooting

### Common Issues
1. **Import Errors**: Ensure all dependencies are installed
2. **Dataset Path**: Use absolute paths or correct relative paths
3. **Memory Issues**: Reduce batch size or use CPU for large models
4. **CUDA Errors**: Install compatible PyTorch version for your GPU

### Performance Tips
- Use GPU for faster training (`--device cuda`)
- Start with smaller experiments (fewer epochs, smaller k)
- Monitor GPU memory usage during hybrid training

## ğŸ“š Research Paper

This implementation corresponds to the research paper:

**"Explanatory Interactive Learning for Bias Mitigation in Visual Gender Classification"**

### Abstract
*Explanatory interactive learning (XIL) enables users to guide model training in machine learning (ML) by providing feedback on the modelâ€™s explanations, thereby helping it to
focus on features that are relevant to the prediction from the userâ€™s perspective. In this study, we explore the capability of this learning paradigm to mitigate bias and spurious correlations in visual classifiers, specifically in a scenario prone to data bias, such as gender classification. We investigate two methodologically different state-of-the-art XIL strategies, i.e., CAIPI and Right for the Right Reasons (RRR), as well as a novel hybrid approach that combines both strategies. The
results are evaluated quantitatively and qualitatively through visual inspection of local explanations provided via Gradient-weighted Class Activation Mapping (GradCAM) and Bounded
Logit Attention (BLA). Experimental results demonstrate the effectiveness of these methods in (i) guiding ML models to focus on relevant image features, particularly when CAIPI is used,
and (ii) reducing model bias (i.e., balancing the misclassification rates between male and female predictions). Our analysis further
supports the potential of XIL methods to improve fairness in gender classifiers. Overall, the increased transparency and fairness obtained by XIL leads to slight performance decreases
with an exception being CAIPI, which shows potential to even improving classification accuracy.*